<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-12-16T14:27:53-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">GPU Puzzlers</title><subtitle>On GPUs, PyTorch and CUDA.</subtitle><author><name>Adnan and Anupam</name></author><entry><title type="html">Vector Flops</title><link href="http://localhost:4000/2022/12/16/vector-flops.html" rel="alternate" type="text/html" title="Vector Flops" /><published>2022-12-16T00:00:00-08:00</published><updated>2022-12-16T00:00:00-08:00</updated><id>http://localhost:4000/2022/12/16/vector-flops</id><content type="html" xml:base="http://localhost:4000/2022/12/16/vector-flops.html">&lt;h2 id=&quot;flops&quot;&gt;Flops&lt;/h2&gt;

&lt;h3 id=&quot;context&quot;&gt;Context&lt;/h3&gt;

&lt;p&gt;Marketing literature for GPUs stresses their high FLOPs. An A100 is advertised as performing 19.5
fp32 TFLOPs - by contrast, the Intel SKL CPU that’s used in T1s comes in at just 921.6 GFLOPs.&lt;/p&gt;

&lt;h3 id=&quot;problem&quot;&gt;Problem&lt;/h3&gt;

&lt;p&gt;This &lt;a href=&quot;/raw/flops_bw.py&quot;&gt;program&lt;/a&gt; does performs a number of numerical operations on tensors: addition,
scalar multiplication, transcendental functions, and matrix multiplications. The &lt;a href=&quot;N=flops.trace.json&quot;&gt;timeline
trace&lt;/a&gt;, shown below, indicates that other than matrix multiplication, all of the
other operations are a tiny fraction (~1%) of the advertised flops. Furthermore, simple operations
like addition take exactly as long as complex ones like sine and log. Why?&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sync_and_pause&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;synchronize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DELAY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;cuda&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mul_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sync_and_pause&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sync_and_pause&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sync_and_pause&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sync_and_pause&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sync_and_pause&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sync_and_pause&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sync_and_pause&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sync_and_pause&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.14159&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sync_and_pause&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sync_and_pause&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;kineto-trace&quot;&gt;Kineto Trace&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/vector_flops/assorted_flops.jpg?raw=true&quot; alt=&quot;Assorted Flops&quot; title=&quot;Assorted Flops&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;hint&quot;&gt;Hint&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Amdahl%27s_law&quot;&gt;Amdahl’s Law&lt;/a&gt;: “the overall performance improvement
gained by optimizing a single part of a system is limited by the fraction of time that the improved
part is actually used”.&lt;/p&gt;

&lt;h3 id=&quot;solution&quot;&gt;Solution&lt;/h3&gt;

&lt;p&gt;Before the GPU can perform numerical operations can on data, that data has to first be read into
registers. The peak memory bandwidth for an A100 is ~2 TB/sec. This means that a 20000x20000 tensor
of fp32 values takes at least (4x20000^2)/2e12 = 0.8ms. Therefore, even if numerical operations were
infinitely fast, the time to perform an elementary operation like multiply by scalar is at least
1.6ms (read the data, write back the updated data), effectively 0.25 TFLOPs, i.e,. 1.3% of the peak.&lt;/p&gt;

&lt;p&gt;This is why all the unary operations take roughly the same time - the number of adds and multiplies
is irrelevant. The binary ops take 50% longer, since there’s 2 Reads and 1 Write per result.&lt;/p&gt;

&lt;p&gt;Matrix multiplication does infact achieve close to advertised performance - 19.1 TFLOPs. This is
because once we read in a value, we operate on it multiple times, thereby amortizing the cost of the
read.&lt;/p&gt;

&lt;h3 id=&quot;discussion&quot;&gt;Discussion&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The ratio of flops performed by an operation to the bytes read/written is known as the compute
intensity of the operation.&lt;/li&gt;
  &lt;li&gt;For the unary and binary operations we saw, the flop count is O(n), where n is the number of
tensor entries; the bytes read/written is also O(n) so the compute intensity is O(1).&lt;/li&gt;
  &lt;li&gt;For matrix multiplication, the flop count is O(n^3), so the compute intensity is O(n^2).&lt;/li&gt;
  &lt;li&gt;The advertised memory bandwidth is misleading: it’s for best-case memory layouts, specifically
when copies are aligned with cache dimensions. It assumes wide transfers, i.e., copying long
contiguous segments of memory - this is true when dealing with tensors, but not for applications
like sorting.&lt;/li&gt;
  &lt;li&gt;Within Meta, the top kernel types are vectorized functors (as we saw above), followed by embedding
bag lookups, followed by matrix multiplication. &lt;img src=&quot;cuda_launch_queue_uarch.jpg?raw=true&quot; alt=&quot;CUDA Launch Queue
Microarchitecture&quot; title=&quot;CUDA Launch Queue Microarchitecture&quot; /&gt;
&lt;!--- from https://slideplayer.com/slide/8211225/ --&gt; &lt;!--- see also
http://xzt102.github.io/publications/2018_GPGPU_Sooraj.pdf --&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Adnan and Anupam</name></author><summary type="html">Flops</summary></entry><entry><title type="html">Launch Queue Delay</title><link href="http://localhost:4000/2022/12/15/launch-queue.html" rel="alternate" type="text/html" title="Launch Queue Delay" /><published>2022-12-15T00:00:00-08:00</published><updated>2022-12-15T00:00:00-08:00</updated><id>http://localhost:4000/2022/12/15/launch-queue</id><content type="html" xml:base="http://localhost:4000/2022/12/15/launch-queue.html">&lt;h2 id=&quot;the-cuda-launch-queue&quot;&gt;The CUDA Launch Queue&lt;/h2&gt;

&lt;h3 id=&quot;context&quot;&gt;Context&lt;/h3&gt;

&lt;p&gt;A kernel is a function that’s executed on the GPU. When the CPU sends a kernel to the GPU, it
doesn’t block on the GPU, i.e., control returns to the CPU thread before the GPU completes the
requested task. This leaves the CPU free for other tasks.&lt;/p&gt;

&lt;h3 id=&quot;problem&quot;&gt;Problem&lt;/h3&gt;

&lt;p&gt;This &lt;a href=&quot;/launch_queue/cuda_launch_queue.py&quot;&gt;program&lt;/a&gt; squares 10 100x100 matrices, followed by squaring a 1600x1600
matrix. After a pause, it does the large squaring, followed by the small squarings. The &lt;a href=&quot;/launch_queue/N=1600-cuda-queue-puzzlers.trace.json&quot;&gt;timeline
trace&lt;/a&gt;, shown below, indicates that it’s faster to do the
large matrix squaring first - why?&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# A is a 1600x1600 matrix, the B[i]s are 10x10 matrices.
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Br&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Ar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;synchronize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Ar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;kineto-trace&quot;&gt;Kineto Trace&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/launch_queue/cuda_launch_queue.jpg?raw=true&quot; alt=&quot;CUDA Launch Queue Trace&quot; title=&quot;CUDA Launch Queue Trace&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;hint&quot;&gt;Hint&lt;/h3&gt;

&lt;p&gt;Since CUDA kernel calls don’t block on the host, GPU operations must be queued up to be executed by
the CUDA driver, as GPU resources become available.&lt;/p&gt;

&lt;h3 id=&quot;solution&quot;&gt;Solution&lt;/h3&gt;

&lt;p&gt;To keep the CPU from blocking when it dispatches compute kernels, the GPU maintains a queue of
kernel calls - the CUDA launch queue - in the order in which they are made by the host.&lt;/p&gt;

&lt;p&gt;It takes time to launch a kernel - the PyTorch dispatch overhead - and this time can dominate the
time taken to perform the actual computation on the GPU. In the first case, the GPU completes each
small matrix multiply before the next one is ready, so it idles between the small multiplies.&lt;/p&gt;

&lt;p&gt;In the second case, the GPU takes longer to perform the large matrix multiply, so the CUDA launch
queue can fill up. After the large matrix multiply finishes, the GPU can immediately turn to the
small matrix multiplies.&lt;/p&gt;

&lt;h3 id=&quot;discussion&quot;&gt;Discussion&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;If we performed more small matrix multiplications after the large one, we would expect at some
point the CUDA launch queue will empty out (since the service rate is higher than the arrival
rate). Empirically, this happens if we have 40 or more small matrix multiplications.&lt;/li&gt;
  &lt;li&gt;Not letting the CUDA launch queue empty out is a guiding principle in designing high performance
PyTorch programs. Some ways to achieve this:
    &lt;ul&gt;
      &lt;li&gt;Operator fusion, wherein we do more work in a single kernel.&lt;/li&gt;
      &lt;li&gt;Avoiding operations that force the queue to be flushed - a common example is a GPU to CPU copy,
which leads to a read-after-write data hazard. (Flushing the queue is analogous to stalling the
pipeline in a pipelined processor.)&lt;/li&gt;
      &lt;li&gt;Reordering independent operations to bring the slower operations to the front of the queue (this
example).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;This graphic shows the launch queues. &lt;img src=&quot;/launch_queue/cuda_launch_queue_uarch.jpg?raw=true&quot; alt=&quot;CUDA Launch Queue
Microarchitecture&quot; title=&quot;CUDA Launch Queue Microarchitecture&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;Each queue entry is constrained to be very small: under 1 KB. It’s basically the function
pointer, and arguments, which are pointers to tensors. Notably, a host-side tensor cannot be an
argument - tensors have to be explicitly copied to and from device.&lt;/li&gt;
      &lt;li&gt;If the CUDA launch queue reaches a threshold (around 1000 entries), the host will block on
calling a compute kernel. This can be problematic if there’s other work the host could be doing,
and should be avoided.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;There are some exceptions to asynchronous kernel launches, notably around CPU-GPU memory copies
and synchronization primitives; we’ll discuss these in another unit.
    &lt;ul&gt;
      &lt;li&gt;Asynchronous launches can be disabled by setting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CUDA_LAUNCH_BLOCKING=1&lt;/code&gt;. This is useful for
debugging, especially in the context of multiple streams - &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#concurrent-execution-host-device&quot;&gt;see the CUDA Toolkit Documentation
for
  details&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In this unit, we’re working with a single
&lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#streams&quot;&gt;stream&lt;/a&gt;. In general the
GPU maintains multiple queues, one per stream. &lt;!--- from https://slideplayer.com/slide/8211225/--&gt;
&lt;!--- see also http://xzt102.github.io/publications/2018_GPGPU_Sooraj.pdf --&gt;
&amp;lt;!–&lt;/li&gt;
  &lt;li&gt;TODO: from Yueming, add NSIGHT traces, understand what is happening there (sending multiple kernels in one shot?)&lt;/li&gt;
  &lt;li&gt;TODO: cudnn optimization enable, see if that leads to pytorch matching CUDA code&lt;/li&gt;
  &lt;li&gt;TODO: summarize jason/kimish insights into launch overhead&lt;/li&gt;
  &lt;li&gt;TODO: see if we can trace PCIE to see how much that contributes and if CUDA graph/CUDA code do group transactions&lt;/li&gt;
  &lt;li&gt;TODO: explain need for Kineto and CUPTI - profiler is not enough
–&amp;gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Adnan and Anupam</name></author><summary type="html">The CUDA Launch Queue</summary></entry><entry><title type="html">Working with latex</title><link href="http://localhost:4000/2022/12/01/math.html" rel="alternate" type="text/html" title="Working with latex" /><published>2022-12-01T00:00:00-08:00</published><updated>2022-12-01T00:00:00-08:00</updated><id>http://localhost:4000/2022/12/01/math</id><content type="html" xml:base="http://localhost:4000/2022/12/01/math.html">&lt;p&gt;Euler’s equation&lt;/p&gt;

\[\begin{equation}
e^{\iota \pi} + 1  = 0
\end{equation}\]

&lt;p&gt;is one of the most interesting equations in mathematics. It contains the five most important
constants $e$, $\iota$, $\pi$, $0$ and $1$.&lt;/p&gt;</content><author><name>Adnan and Anupam</name></author><summary type="html">Euler’s equation</summary></entry></feed>