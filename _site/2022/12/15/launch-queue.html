<!DOCTYPE html>
<html lang="en">  <script>
  MathJax = {
    loader: {load: ['[tex]/textmacros']},
    tex: {
      packages: {'[+]': ['textmacros'] },            // extensions to use
      inlineMath: [['$','$'], ['\\(', '\\)']],       // start/end delimiter pairs for in-line math
      displayMath: [ ['$$', '$$'], ['\\[', '\\]'] ], // start/end delimiter pairs for delimiter math
      processEscapes: false,                          // use \$ to produce a literal dollar sign
      processEnvironments: true,                     // process \begin{xxx}...\end{xxx} outside math mode
      processRefs: true,                             // process \ref{...} outside of math mode
      digits: /^(?:[0-9]+(?:\{,\}[0-9]{3})*(?:\.[0-9]*)?|\.[0-9]+)/,  // pattern for recognizing numbers
      tags: 'ams',                                   // or 'ams' or 'all'
      tagSide: 'right',                              // side for \tag macros
      tagIndent: '0.8em',                            // amount to indent tags
      useLabelIds: true,                             // use label name rather than tag for ids
      maxMacros: 1000,                               // maximum number of macro substitutions per expression
      maxBuffer: 5 * 1024,                           // maximum size for the internal TeX string (5K)
      formatError: (jax, err) => jax.formatError(err) // function called when TeX syntax errors occur
    }
  };
</script>

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Launch Queue Delay | GPU Puzzlers</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Launch Queue Delay" />
<meta name="author" content="Adnan and Anupam" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The CUDA Launch Queue" />
<meta property="og:description" content="The CUDA Launch Queue" />
<link rel="canonical" href="http://localhost:4000/2022/12/15/launch-queue.html" />
<meta property="og:url" content="http://localhost:4000/2022/12/15/launch-queue.html" />
<meta property="og:site_name" content="GPU Puzzlers" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-12-15T00:00:00-08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Launch Queue Delay" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Adnan and Anupam"},"dateModified":"2022-12-15T00:00:00-08:00","datePublished":"2022-12-15T00:00:00-08:00","description":"The CUDA Launch Queue","headline":"Launch Queue Delay","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2022/12/15/launch-queue.html"},"url":"http://localhost:4000/2022/12/15/launch-queue.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="GPU Puzzlers" />
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">GPU Puzzlers</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Launch Queue Delay</h1>
    <p class="post-meta"><time class="dt-published" datetime="2022-12-15T00:00:00-08:00" itemprop="datePublished">
        Dec 15, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="the-cuda-launch-queue">The CUDA Launch Queue</h2>

<h3 id="context">Context</h3>

<p>A kernel is a function that’s executed on the GPU. When the CPU sends a kernel to the GPU, it
doesn’t block on the GPU, i.e., control returns to the CPU thread before the GPU completes the
requested task. This leaves the CPU free for other tasks.</p>

<h3 id="problem">Problem</h3>

<p>This <a href="/launch_queue/cuda_launch_queue.py">program</a> squares 10 100x100 matrices, followed by squaring a 1600x1600
matrix. After a pause, it does the large squaring, followed by the small squarings. The <a href="/launch_queue/N=1600-cuda-queue-puzzlers.trace.json">timeline
trace</a>, shown below, indicates that it’s faster to do the
large matrix squaring first - why?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># A is a 1600x1600 matrix, the B[i]s are 10x10 matrices.
</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">Br</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">],</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
<span class="n">Ar</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">A</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">synchronize</span><span class="p">()</span>
<span class="n">time</span><span class="p">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">)</span>

<span class="n">Ar</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">A</span><span class="p">)</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">],</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
</code></pre></div></div>

<h4 id="kineto-trace">Kineto Trace</h4>
<p><img src="/launch_queue/cuda_launch_queue.jpg?raw=true" alt="CUDA Launch Queue Trace" title="CUDA Launch Queue Trace" /></p>

<h3 id="hint">Hint</h3>

<p>Since CUDA kernel calls don’t block on the host, GPU operations must be queued up to be executed by
the CUDA driver, as GPU resources become available.</p>

<h3 id="solution">Solution</h3>

<p>To keep the CPU from blocking when it dispatches compute kernels, the GPU maintains a queue of
kernel calls - the CUDA launch queue - in the order in which they are made by the host.</p>

<p>It takes time to launch a kernel - the PyTorch dispatch overhead - and this time can dominate the
time taken to perform the actual computation on the GPU. In the first case, the GPU completes each
small matrix multiply before the next one is ready, so it idles between the small multiplies.</p>

<p>In the second case, the GPU takes longer to perform the large matrix multiply, so the CUDA launch
queue can fill up. After the large matrix multiply finishes, the GPU can immediately turn to the
small matrix multiplies.</p>

<h3 id="discussion">Discussion</h3>

<ul>
  <li>If we performed more small matrix multiplications after the large one, we would expect at some
point the CUDA launch queue will empty out (since the service rate is higher than the arrival
rate). Empirically, this happens if we have 40 or more small matrix multiplications.</li>
  <li>Not letting the CUDA launch queue empty out is a guiding principle in designing high performance
PyTorch programs. Some ways to achieve this:
    <ul>
      <li>Operator fusion, wherein we do more work in a single kernel.</li>
      <li>Avoiding operations that force the queue to be flushed - a common example is a GPU to CPU copy,
which leads to a read-after-write data hazard. (Flushing the queue is analogous to stalling the
pipeline in a pipelined processor.)</li>
      <li>Reordering independent operations to bring the slower operations to the front of the queue (this
example).</li>
    </ul>
  </li>
  <li>This graphic shows the launch queues. <img src="/launch_queue/cuda_launch_queue_uarch.jpg?raw=true" alt="CUDA Launch Queue
Microarchitecture" title="CUDA Launch Queue Microarchitecture" />
    <ul>
      <li>Each queue entry is constrained to be very small: under 1 KB. It’s basically the function
pointer, and arguments, which are pointers to tensors. Notably, a host-side tensor cannot be an
argument - tensors have to be explicitly copied to and from device.</li>
      <li>If the CUDA launch queue reaches a threshold (around 1000 entries), the host will block on
calling a compute kernel. This can be problematic if there’s other work the host could be doing,
and should be avoided.</li>
    </ul>
  </li>
  <li>There are some exceptions to asynchronous kernel launches, notably around CPU-GPU memory copies
and synchronization primitives; we’ll discuss these in another unit.
    <ul>
      <li>Asynchronous launches can be disabled by setting <code class="language-plaintext highlighter-rouge">CUDA_LAUNCH_BLOCKING=1</code>. This is useful for
debugging, especially in the context of multiple streams - <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#concurrent-execution-host-device">see the CUDA Toolkit Documentation
for
  details</a>.</li>
    </ul>
  </li>
  <li>In this unit, we’re working with a single
<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#streams">stream</a>. In general the
GPU maintains multiple queues, one per stream. <!--- from https://slideplayer.com/slide/8211225/-->
<!--- see also http://xzt102.github.io/publications/2018_GPGPU_Sooraj.pdf -->
&lt;!–</li>
  <li>TODO: from Yueming, add NSIGHT traces, understand what is happening there (sending multiple kernels in one shot?)</li>
  <li>TODO: cudnn optimization enable, see if that leads to pytorch matching CUDA code</li>
  <li>TODO: summarize jason/kimish insights into launch overhead</li>
  <li>TODO: see if we can trace PCIE to see how much that contributes and if CUDA graph/CUDA code do group transactions</li>
  <li>TODO: explain need for Kineto and CUPTI - profiler is not enough
–&gt;</li>
</ul>

  </div><a class="u-url" href="/2022/12/15/launch-queue.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <ul class="contact-list">
          <li class="p-name">Adnan and Anupam</li>
          
        </ul>
      </div>
      <div class="footer-col">
        <p>On GPUs, PyTorch and CUDA.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/adnanaziz/GPUPuzzlers" target="_blank" title="GPUPuzzlers"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
