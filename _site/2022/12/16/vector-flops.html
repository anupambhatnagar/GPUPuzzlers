<!DOCTYPE html>
<html lang="en">  <script>
  MathJax = {
    loader: {load: ['[tex]/textmacros']},
    tex: {
      packages: {'[+]': ['textmacros'] },            // extensions to use
      inlineMath: [['$','$'], ['\\(', '\\)']],       // start/end delimiter pairs for in-line math
      displayMath: [ ['$$', '$$'], ['\\[', '\\]'] ], // start/end delimiter pairs for delimiter math
      processEscapes: false,                          // use \$ to produce a literal dollar sign
      processEnvironments: true,                     // process \begin{xxx}...\end{xxx} outside math mode
      processRefs: true,                             // process \ref{...} outside of math mode
      digits: /^(?:[0-9]+(?:\{,\}[0-9]{3})*(?:\.[0-9]*)?|\.[0-9]+)/,  // pattern for recognizing numbers
      tags: 'ams',                                   // or 'ams' or 'all'
      tagSide: 'right',                              // side for \tag macros
      tagIndent: '0.8em',                            // amount to indent tags
      useLabelIds: true,                             // use label name rather than tag for ids
      maxMacros: 1000,                               // maximum number of macro substitutions per expression
      maxBuffer: 5 * 1024,                           // maximum size for the internal TeX string (5K)
      formatError: (jax, err) => jax.formatError(err) // function called when TeX syntax errors occur
    }
  };
</script>

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Vector Flops | GPU Puzzlers</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Vector Flops" />
<meta name="author" content="Adnan and Anupam" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Flops" />
<meta property="og:description" content="Flops" />
<link rel="canonical" href="http://localhost:4000/2022/12/16/vector-flops.html" />
<meta property="og:url" content="http://localhost:4000/2022/12/16/vector-flops.html" />
<meta property="og:site_name" content="GPU Puzzlers" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-12-16T00:00:00-08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Vector Flops" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Adnan and Anupam"},"dateModified":"2022-12-16T00:00:00-08:00","datePublished":"2022-12-16T00:00:00-08:00","description":"Flops","headline":"Vector Flops","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2022/12/16/vector-flops.html"},"url":"http://localhost:4000/2022/12/16/vector-flops.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="GPU Puzzlers" />
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">GPU Puzzlers</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Vector Flops</h1>
    <p class="post-meta"><time class="dt-published" datetime="2022-12-16T00:00:00-08:00" itemprop="datePublished">
        Dec 16, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="flops">Flops</h2>

<h3 id="context">Context</h3>

<p>Marketing literature for GPUs stresses their high FLOPs. An A100 is advertised as performing 19.5
fp32 TFLOPs - by contrast, the Intel SKL CPU that’s used in T1s comes in at just 921.6 GFLOPs.</p>

<h3 id="problem">Problem</h3>

<p>This <a href="/raw/flops_bw.py">program</a> does performs a number of numerical operations on tensors: addition,
scalar multiplication, transcendental functions, and matrix multiplications. The <a href="N=flops.trace.json">timeline
trace</a>, shown below, indicates that other than matrix multiplication, all of the
other operations are a tiny fraction (~1%) of the advertised flops. Furthermore, simple operations
like addition take exactly as long as complex ones like sine and log. Why?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sync_and_pause</span><span class="p">():</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">time</span><span class="p">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">DELAY</span><span class="p">)</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span><span class="p">))</span>

<span class="n">A</span><span class="p">.</span><span class="n">mul_</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">sync_and_pause</span><span class="p">()</span>

<span class="n">B</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="n">mul</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">sync_and_pause</span><span class="p">()</span>

<span class="n">B</span> <span class="o">+=</span> <span class="n">A</span>
<span class="n">sync_and_pause</span><span class="p">()</span>

<span class="n">C</span> <span class="o">=</span> <span class="n">B</span> <span class="o">+</span> <span class="n">A</span>
<span class="n">sync_and_pause</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="n">sync_and_pause</span><span class="p">()</span>

    <span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="n">sync_and_pause</span><span class="p">()</span>

    <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">B</span><span class="p">)</span>
    <span class="n">sync_and_pause</span><span class="p">()</span>

    <span class="n">torch</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">B</span><span class="p">)</span>
    <span class="n">sync_and_pause</span><span class="p">()</span>

    <span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="mf">3.14159</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">B</span><span class="p">)</span>
    <span class="n">sync_and_pause</span><span class="p">()</span>

    <span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
    <span class="n">sync_and_pause</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="kineto-trace">Kineto Trace</h4>
<p><img src="/vector_flops/assorted_flops.jpg?raw=true" alt="Assorted Flops" title="Assorted Flops" /></p>

<h3 id="hint">Hint</h3>

<p><a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl’s Law</a>: “the overall performance improvement
gained by optimizing a single part of a system is limited by the fraction of time that the improved
part is actually used”.</p>

<h3 id="solution">Solution</h3>

<p>Before the GPU can perform numerical operations can on data, that data has to first be read into
registers. The peak memory bandwidth for an A100 is ~2 TB/sec. This means that a 20000x20000 tensor
of fp32 values takes at least (4x20000^2)/2e12 = 0.8ms. Therefore, even if numerical operations were
infinitely fast, the time to perform an elementary operation like multiply by scalar is at least
1.6ms (read the data, write back the updated data), effectively 0.25 TFLOPs, i.e,. 1.3% of the peak.</p>

<p>This is why all the unary operations take roughly the same time - the number of adds and multiplies
is irrelevant. The binary ops take 50% longer, since there’s 2 Reads and 1 Write per result.</p>

<p>Matrix multiplication does infact achieve close to advertised performance - 19.1 TFLOPs. This is
because once we read in a value, we operate on it multiple times, thereby amortizing the cost of the
read.</p>

<h3 id="discussion">Discussion</h3>

<ul>
  <li>The ratio of flops performed by an operation to the bytes read/written is known as the compute
intensity of the operation.</li>
  <li>For the unary and binary operations we saw, the flop count is O(n), where n is the number of
tensor entries; the bytes read/written is also O(n) so the compute intensity is O(1).</li>
  <li>For matrix multiplication, the flop count is O(n^3), so the compute intensity is O(n^2).</li>
  <li>The advertised memory bandwidth is misleading: it’s for best-case memory layouts, specifically
when copies are aligned with cache dimensions. It assumes wide transfers, i.e., copying long
contiguous segments of memory - this is true when dealing with tensors, but not for applications
like sorting.</li>
  <li>Within Meta, the top kernel types are vectorized functors (as we saw above), followed by embedding
bag lookups, followed by matrix multiplication. <img src="cuda_launch_queue_uarch.jpg?raw=true" alt="CUDA Launch Queue
Microarchitecture" title="CUDA Launch Queue Microarchitecture" />
<!--- from https://slideplayer.com/slide/8211225/ --> <!--- see also
http://xzt102.github.io/publications/2018_GPGPU_Sooraj.pdf --></li>
</ul>


  </div><a class="u-url" href="/2022/12/16/vector-flops.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <ul class="contact-list">
          <li class="p-name">Adnan and Anupam</li>
          
        </ul>
      </div>
      <div class="footer-col">
        <p>On GPUs, PyTorch and CUDA.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/adnanaziz/GPUPuzzlers" target="_blank" title="GPUPuzzlers"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
